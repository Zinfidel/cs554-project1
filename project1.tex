\documentclass{article}
\usepackage{minted}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage[left=1in, right=1in]{geometry}

\begin{document}
\title{Project 1 Report}
\author{Taylor Berger, Zachary Friedland, Jianyu Yang}
\maketitle

\section{Language Decisions}
\paragraph{} We decided to write our project in Python due to the ease of
expressing high level concepts and the removal of memory management
from the project. We found it to be an effective choice since we could
focus more on algorithmic nature of the project instead of the minutae
of memory manipulation and management. At the time of writing this
paper, we were using version 2.7.2 on a unix based system.

\paragraph{} We also used an external library that will have to be installed
called Pyparsing. You can get more information about this in the Appendix 
section~\ref{sec:libaries}.

\section{Alphabets}
\label{sec:alphabet}
\paragraph{} All alphabets in this project are restricted to the printable set of
ASCII characters although a general alphabet may be completely
unrestricted. An alphabet is defined as a set of unique symbols and are formally represented as:

\[\Sigma = \{\sigma_1, \sigma_2, ... ,\sigma_n\}\]

\paragraph{} In is project, alphabets are given to us in text only format.
 Each obect in the alphabet is a pre-quoted symbol preceeded by the
 word \verb|alphabet| and followed by the word \verb|end|. An example
 alphabet containing the symbols $a,b,c$ would be:

\begin{verbatim}
alphabet
'a 'b 'c
end
\end{verbatim}

\paragraph{} The former example can be abstracted into a formal definition:
\begin{verbatim}
Alphabet -> alphabet_keyword AlphabetList end_keyword
AlphabetList -> 
AlphabetList -> Sigma AlphabetList
alphabet_keywork -> 'alphabet
end_keyword -> 'end
\end{verbatim}

\paragraph{}To form an alphabet in our project, we parse the file using pyparsing
(see appendix). The following code was be used to parse any arbitrary
set of pre-quoted symbols into a list of symbols that we assumed make up
an alphabet.

\begin{minted}[linenos=true]{python}
from pyparsing import *

# Alphabet definition
alphabet_keyword = Keyword("alphabet").suppress()
alphabet_end_keyword = Keyword("end;").suppress() |\
                       Keyword("end").suppress()
Symbol = Combine(Literal("\'").suppress() +\
                 Optional(Literal("\\")) +\ 
                 Word(printables + " ", exact=1))

Symbol.setParseAction(decodeEscapes)
SymbolList = OneOrMore(Symbol)
Alphabet = alphabet_keyword + SymbolList + alphabet_end_keyword
\end{minted}

\paragraph{} Pyparsing's utility function \verb|suppress| allows the 
parser to expect the value and remove it from the output completely
after a successful parse is compelted. Also, the
\verb|setParseAction(decodeEscapes)| function was used on line 10 to
amake sure single characters that needed to be escaped in printable
ASCII (looking at you \verb|\n|) were parsed into their correct form
instead of a two character sequence starting with the backslash.

\section{Description of Non-Deterministic Finite State Automata (NFA)}
\label{sec:nfa}
A formal definition for the NFA can be viewed as the following:
\begin{verbatim}
Nfa -> nfa_keywork States InitialStates AcceptingStates Transitions
States -> states_keywork StateList end_keyword
StateList -> 
StateList -> State StateList
InitialState -> initial_keyword State
AcceptingStates -> accept_keyword StateList end_keyword
Transitions -> transitions_keyword TransitionList end_keyword
TransitionList ->
TransitionList -> Transition TransitionList
Transition -> State SymbolList arrow State
SymbolList ->
SymbolList -> symbol SymbolList
dfa_keyword -> 'dfa
states_keyword -> 'states
initial_keyword -> 'initial
accept_keyword -> 'accept
transitions_keyword -> 'transitions
end_keyword -> 'end;
arrow -> '-->
\end{verbatim}

\paragraph{} It should be mentioned here that \verb|symbol| $\in \Sigma$, 
where $\Sigma$ is the alphabet that corresponds to this NFA.

\section{Data Structures}
\label{sec:algos}
\paragraph{} Since the algorithms depend on how the data structure it 
operates on is constructed, we will cover the three main data
structures (regular expressions, and DFA/NFAs) used in this project
first.

\subsection{Regular Expressions}
\label{sec:algos:regex}
\subsubsection{Production}
\label{sec:algos:regex:production}
We begin by defining a base class in which all other types of regular
expressions inherit from. This class is meaningless except to give
the rest of the classes a common ancestor.

\begin{minted}{python}
class Production():
    '''
    Defines the base class that all Regex inherit from. 
    '''
    def __init__(self):
        pass

    def matches(self, string):
        pass

    def consume(self, string):
        pass
\end{minted}

\paragraph{} We define two methods for this class, \verb|matches| 
and \verb|consume| where they will, respectively, return true if they
match the parameterized string and consume as much off the string as
possible as long as they match what they consume. These two functions
vary depending on the implementation so they must be overridden in
their subclasses.

\subsubsection{Sigma}
\label{sec:algos:regex:sigma}
\paragraph{} The first regular expression we define is the simplest, 
but most important, Sigma production. This is a regular expression
that is responsible for matching to a single character and is a terminal 
regular expression for a language and is defined as \verb|E -> |$\sigma$ where $\sigma$ is a part of a predefined alphabet described in section~\ref{sec:alphabet}. 
\begin{minted}{python}
class Sigma(Production):
    def __init__(self, sigma):
        self.sigma = sigma

    def __str__(self):
        return str(self.sigma)

    def matches(self, string):
        return self.sigma == string

    def consume(self, string):
        if len(string) >= 1 and string[0] == self.sigma:
            return string[0:1], string[1:]
        else:
            return '', string
\end{minted}

\subsubsection{Repetition}
\label{sec:algos:regex:rep}
\paragraph{} The next type of regular expression to implement was the 
repetition expression, or Kleene closure defined as \verb|E -> * E|. 

\begin{minted}{python}
class Repetition(Production):
    def __init__(self, expr):
        self.expr = expr
    
    def __str__(self):
        return "* " + str(self.expr)

    def matches(self, string):
        if string == '':
            return True

        return self.expr.matches(string[0:1]) and self.matches(string[1:])
        
    def consume(self, string):
        consumed = 'default'
        total_consumed = ''
        leftover = string

        while consumed != '':
            consumed, leftover = self.expr.consume(leftover)
            total_consumed += consumed

        return total_consumed, leftover
\end{minted}

\subsubsection{Alternative}
\paragraph{} Implementint the alternative production required the composition 
of multiple regular expressions since it is a binary operator. Alternative
regular expressions take the form: 

\begin{verbatim}
E -> | E E
\end{verbatim}

and are represented in our code as:

\begin{minted}{python}
class Alternative(Production):
    def __init__(self, left, right):
        self.left = left
        self.right = right

    def __str__(self):
        return '| ' + str(self.left) + ' ' +  str(self.right)

    def matches(self, string):
        return self.left.matches(string) or \
               self.right.matches(string)

    def consume(self, string):
        left_consume, leftover = self.left.consume(string)
        # he he he. rightover.... I crack myself up.
        right_consume, rightover = self.right.consume(string)

        if len(left_consume) >= len(right_consume):
            return left_consume, leftover
        else:
            return right_consume, rightover

\end{minted}

\subsubsection{Concatenation}
\label{sec:algos:regex:concat}
\paragraph{} Concatenations are formally represented in the form \verb|E -> + E E| and are the last meaningful regular expression we need to be able to construct. We represented them as follows.

\begin{minted}{python}
class Concatenation(Production):
    def __init__(self, left, right):
        self.left = left
        self.right = right
    
    def __str__(self):
        return '+ ' + str(self.left) + ' ' + str(self.right)

    def matches(self, string):
        return self.left.matches(string[0:1]) and self.right.matches(string[1:])
    
    def consume(self, string):
        left_match, leftover = self.left.consume(string)
        right_match, leftover = self.right.consume(leftover)

        if left_match == '':
            return '', string
 
        left_match += right_match

        return ''.join(left_match), leftover
\end{minted}

\subsubsection{Nil Expression}
\paragraph{} For completeness, a regular expression has one other 
semantically correct production, the Nil Expression. This expression
recognizes regular expressions of the type \verb|E -> _| where the
underscore represents the empty string.

\begin{minted}{python}
class NilExpression(Production):
    def __str__(self):
        # return ''
        return 'ε'

    def __repr__(self):
        return 'ε'

    def matches(self, string):
        return string == ''

    def consume(self, string):
        return '', string
\end{minted}

\subsection{Automata}
\paragraph{} For our project, since the main difference between
a NFA and a DFA is the symbol list for a transition instead of just a
single symbol, we chose to keep the data structure the same and only
interpret them differently when requried to by a specific algorithm.

\paragraph{} The basic structure for the Automata class is a directed
graph object. A dictionary is used to identify edges from one node to
the next where edges are indexed on the transition character. This is
where the NFA and DFA must be interpretted differently. For the DFA,
we expect the number of states returned to be either 0 or 1. However
for the NFA, any number of states may be returned upon inspected the
transition edges based on a certain character $\sigma$.

\paragraph{} The full implementation for an automata can be found in section~\ref{code:automata}.

\subsection{Construction of Scanners}
\paragraph{} The interface between the the scanner and a parser is simply a list of matched tokens defined by the token class in scanner.py. The list of tokens are in the order they have been found. Tokens are defined as the following:

\begin{minted}{python}
class Token:
    '''Essentially a tuple of a String, LexicalClass, and a relevance as
       defined in the lexical desciption of the grammar
    '''

    def __init__(self, string, lex_class_name, relevance):
        self.string = string
        self.lexical_class = lex_class_name
        self.relevance = relevance

    def __str__(self):
        return "Class: " + str(self.lexical_class) + "\n\tString: "\
               + str(self.string)
\end{minted}

\paragraph{} To construct a parser, a formal definition of a language can be
given to the BuildLexicalDescription function found in
description\_reader.py. The function returns a Scanner object with a
single function called \verb|parse|. The parse function takes a string
and returns a list of Tokens identified in the language or raises an
exception if the parse fails (meaning the string is not in the language).

\section{Algorithms}
\paragraph{}All algorithms were written in stand alone files that
operated on parameters passed into their function calls. Please see
section~\ref{code} for the individual algorithm implementations.

\section{Complete Lexical Descriptions}
\paragraph{}For any complete lexical description we can construct a 
parser that will return a list of sequential strings recognized by the
language. For our implemenation, we shied away from using the massive
DFA for token recognition since the regular expression encoded enough
information for token recognition. 

\begin{minted}{python}
class LexicalDesc:
  #... ommitted for clarity sake

        def scan(self, string_to_scan, tokens=[]):

        # ... comments ommitted ...

        if string_to_scan == '':
            return tokens

        '''
           Represents the leftmost parse of the parse tree.
        '''
        for c in self.classes:
            matched, leftover = c.regex.consume(string_to_scan)

            #if we do get a match using the regex
            if matched != '':
                if c.relevance != 'discard':
                    new_tokens = tokens + [Token(matched, c.name, c.relevance)]
                else:
                    new_tokens = tokens

                try:
                    return self.scan(leftover, new_tokens)
                except Exception as e:
                    continue

        ''' 
            If we get here, that means there was no logical parse
            using the regexes we were given. We should return an error
            at this point. 
        '''
        raise Exception(string_to_scan)
\end{minted}

\paragraph{} Since it is a recursive definition, we constantly scan a
smaller and smaller parse from the string until we either run out of
string to parse (success) or run out of options when trying to consume
using the regexes (failure). If at the top level, we raise an
exception we know that there were no logical parses for the string
using the regex of the lexical description and therefore the string is
not a part of the language.


\paragraph{} However, it should be noted that a lexical description
of the file can be converted into a union of regular expressions. The
regular expressions can then be converted into a (very) large NFA
which can then be converted into an even larger DFA. That DFA can also
be used to consume input from the front on an input stream and every
time it reaches an accept state (that contains some meta information),
we can output the total consumed characters up to that point as a
single token.  It should also be noted that you can convert a DFA to a
regular expression, so our method is equivalent to parsing using a
DFA.

\section{Testing and Integration}
\paragraph{} To test the completeness of the program and the requirements
testing.py was created and three test cases were created. Test case 0
tested the scanning and tokenizing aspect of the scanner generator.
Test case 1 and 2 work with converting regular expressions to NFAs and
NFAs to DFAs and then minimizing the resulting DFA. See
section~\ref{code:testing}.

\paragraph{} When empirically comparing the combination of subset 
constuction and Hopcroft's algorithm, we found that on a NFA constructed
from a regular expression of the form:
\begin{verbatim}
class integerArithmatic is  + * | '1 | '2 | '3 | '4 | '5 | '6 | '7 | '8 |
                            '9 '0 + | '* | '/ | '+ '- * | '1 | '2 | '3 | 
                            '4 | '5 | '6 | '7 | '8 | '9 '0  relevant end;
\end{verbatim}
ran in less than a second's worth of time. However when we ran
Brzozowski's algorithm, the computation didn't seem to end even after
waiting several minutes. Other NFA's constructed from regular expressions
seemed to share the same results, Hopcroft's finished in relatively
little time whereas Brzozowski's regularly failed finish in a reasonable
amount of time.

\section{Team Composition}
\paragraph{}Taylor was responsible for constructing all class structures, 
putting it all together in the scanner generator, as well as writing
the report. Jianyu and Zach wrote many of the algorithms used and Zach
put in a considerable amount of time using Pyparsing to read our input
files into the correct format.

\paragraph{}It did take a while for our group to fully understand the
project specification but once we understood where we needed to go,
the rest of the project was fairly straightforward. We actually found
out we were missing a large chunk of the final portion (scanner
generator) a few days before turn-in but were able to group together
and write the code we needed to successfully complete the assignment.

\section{Appendix: Libraries Used}
\label{sec:libaries}
\paragraph{} We tried to stay away from using any libraries that would make
tour required tasks trivial except for reading input files. For that
particular task we opted to use a Python text parser called
\href{http://pyparsing.wikispaces.com/}{Pyparsing}. You can download
and install via the python egg from their website.

\section{Source Files}
\label{code}
\subsection{regex.py}
\label{code:regex}
\inputminted{python}{./regex.py}

\subsection{automata.py}
\label{code:automata}
\inputminted{python}{./automata.py}

\subsection{scanner.py}
\label{code:scanner}
\inputminted{python}{./scanner.py}

\subsection{thompsons\_construction.py}
\label{code:thompsons}
\inputminted{python}{./thompsons_construction.py}

\subsection{subset\_construction.py}
\label{code:subset}
\inputminted{python}{./subset_construction.py}

\subsection{dfa\_to\_re.py}
\label{code:dfaToRe}
\inputminted{python}{./dfa_to_re.py}

\subsection{hopcrofts\_algorithm.py}
\label{code:hopcrofts}
\inputminted{python}{./hopcrofts_algorithm.py}

\subsection{dfa\_read.py}
\label{code:dfaRead}
\inputminted{python}{./dfa_read.py}

\subsection{brzozowski.py}
\label{code:brzo}
\inputminted{python}{./brzozowski.py}

\subsection{description\_reader.py}
\label{code:descriptionReader}
\inputminted{python}{./description_reader.py}

\subsection{testing.py}
\label{code:testing}
\inputminted{python}{./testing.py}

\section{Link to Code}
Code can be found at \href{https://github.com/zinfidel/cs554-project1}{Zach's github}.

\end {document}
